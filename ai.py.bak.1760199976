import os
import json
import csv
import gzip
import subprocess
import shutil
from pathlib import Path
import numpy as np

# Optional PyTorch import
TORCH_AVAILABLE = True
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader, Dataset
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
except Exception:
    TORCH_AVAILABLE = False
    torch = nn = F = DataLoader = Dataset = DEVICE = None

# Piece values and PSTs (same as earlier)
piece_values = {'P':100, 'N':320, 'B':330, 'R':500, 'Q':900, 'K':20000}

pawn_table = [
     0,0,0,0,0,0,0,0,
    50,50,50,50,50,50,50,50,
    10,10,20,30,30,20,10,10,
     5,5,10,25,25,10,5,5,
     0,0,0,20,20,0,0,0,
     5,-5,-10,0,0,-10,-5,5,
     5,10,10,-20,-20,10,10,5,
     0,0,0,0,0,0,0,0
]

knight_table = [
   -50,-40,-30,-30,-30,-30,-40,-50,
   -40,-20,0,0,0,0,-20,-40,
   -30,0,10,15,15,10,0,-30,
   -30,5,15,20,20,15,5,-30,
   -30,0,15,20,20,15,0,-30,
   -30,5,10,15,15,10,5,-30,
   -40,-20,0,5,5,0,-20,-40,
   -50,-40,-30,-30,-30,-30,-40,-50
]

bishop_table = [
   -20,-10,-10,-10,-10,-10,-10,-20,
   -10,5,0,0,0,0,5,-10,
   -10,10,10,10,10,10,10,-10,
   -10,0,10,10,10,10,0,-10,
   -10,5,5,10,10,5,5,-10,
   -10,0,5,10,10,5,0,-10,
   -10,0,0,0,0,0,0,-10,
   -20,-10,-10,-10,-10,-10,-10,-20
]

rook_table = [
     0,0,0,0,0,0,0,0,
     5,10,10,10,10,10,10,5,
    -5,0,0,0,0,0,0,-5,
    -5,0,0,0,0,0,0,-5,
    -5,0,0,0,0,0,0,-5,
    -5,0,0,0,0,0,0,-5,
    -5,0,0,0,0,0,0,-5,
     0,0,0,5,5,0,0,0
]

queen_table = [
   -20,-10,-10,-5,-5,-10,-10,-20,
   -10,0,0,0,0,0,0,-10,
   -10,0,5,5,5,5,0,-10,
    -5,0,5,5,5,5,0,-5,
     0,0,5,5,5,5,0,-5,
   -10,5,5,5,5,5,0,-10,
   -10,0,5,0,0,0,0,-10,
   -20,-10,-10,-5,-5,-10,-10,-20
]

king_table = [
   -30,-40,-40,-50,-50,-40,-40,-30,
   -30,-40,-40,-50,-50,-40,-40,-30,
   -30,-40,-40,-50,-50,-40,-40,-30,
   -30,-40,-40,-50,-50,-40,-40,-30,
   -20,-30,-30,-40,-40,-30,-30,-20,
   -10,-20,-20,-20,-20,-20,-20,-10,
    20,20,0,0,0,0,20,20,
    20,30,10,0,0,10,30,20
]

# Static Exchange Evaluation (local)
def static_exchange_eval_local(game, target, colour, move_cache):
    if not game.board[target[0]][target[1]]:
        return 0
    copyg = game.light_copy()
    gains = [piece_values[copyg.board[target[0]][target[1]].name]]
    side = colour
    MAX_STEPS = 32
    steps = 0
    while steps < MAX_STEPS:
        steps += 1
        attackers = []
        for r in range(8):
            for c in range(8):
                piece = copyg.board[r][c]
                if not piece or piece.colour != side:
                    continue
                moves = move_cache.get((r,c), copyg.get_moves(r,c))
                if target in moves:
                    attackers.append((r,c))
        if not attackers:
            break
        best_attacker = min(attackers, key=lambda sq: piece_values[copyg.board[sq[0]][sq[1]].name])
        copyg.make_move(best_attacker, target)
        side = 'b' if side == 'w' else 'w'
        if copyg.board[target[0]][target[1]]:
            gains.append(piece_values[copyg.board[target[0]][target[1]].name] - gains[-1])
        else:
            gains.append(-gains[-1])
    for i in range(len(gains)-2, -1, -1):
        gains[i] = max(-gains[i+1], gains[i])
    return gains[0]

# Board evaluation
def evaluate_board(game, move_cache=None):
    if move_cache is None:
        move_cache = {}
    score = 0
    white_pawns, black_pawns = [], []
    for row in range(8):
        for col in range(8):
            piece = game.board[row][col]
            if not piece:
                continue
            val = piece_values[piece.name]
            index = row*8 + col if piece.colour == 'w' else (7-row)*8 + (7-col)
            if piece.name == 'P': val += pawn_table[index]
            elif piece.name == 'N': val += knight_table[index]
            elif piece.name == 'B': val += bishop_table[index]
            elif piece.name == 'R': val += rook_table[index]
            elif piece.name == 'Q': val += queen_table[index]
            elif piece.name == 'K': val += king_table[index]
            if piece.name == 'P':
                (white_pawns if piece.colour=='w' else black_pawns).append((row,col))
            score += val if piece.colour=='w' else -val
    def pawn_structure(pawns, enemy_pawns, colour):
        bonus = 0
        files = [0]*8
        for r,c in pawns:
            files[c] += 1
        for f in range(8):
            if files[f]>1: bonus -= 20*(files[f]-1)
            if f==0 and files[f+1]==0: bonus -= 15
            elif f==7 and files[f-1]==0: bonus -= 15
            elif 0<f<7 and files[f-1]==0 and files[f+1]==0: bonus -= 15
        for r,c in pawns:
            blocked = any(ec in [c-1,c,c+1] and ((colour=='w' and er<r) or (colour=='b' and er>r)) for er,ec in enemy_pawns)
            if not blocked:
                rank = r if colour=='b' else 7-r
                bonus += 10 + rank*5
        return bonus
    score += pawn_structure(white_pawns, black_pawns,'w')
    score -= pawn_structure(black_pawns, white_pawns,'b')
    return score

# Minimax with SSE
def minimax_sse(game, depth, alpha, beta, maximizing, original_depth=None, move_cache=None):
    if original_depth is None:
        original_depth = depth
    if move_cache is None:
        move_cache = {}
    if depth == 0 or game.state is not None:
        return evaluate_board(game, move_cache)
    best_move = None
    if maximizing:
        max_eval = float('-inf')
        for r in range(8):
            for c in range(8):
                piece = game.board[r][c]
                if not piece or piece.colour != 'w': continue
                moves = move_cache.get((r,c), game.get_moves(r,c))
                for mv in moves:
                    if game.board[mv[0]][mv[1]] and static_exchange_eval_local(game, mv, 'w', move_cache) < 0:
                        continue
                    copy = game.light_copy()
                    copy.make_move((r,c), mv)
                    val = minimax_sse(copy, depth-1, alpha, beta, False, original_depth, move_cache)
                    if val > max_eval:
                        max_eval = val; best_move = ((r,c), mv)
                    alpha = max(alpha, val)
                    if beta <= alpha: break
        return best_move if depth == original_depth else max_eval
    else:
        min_eval = float('inf')
        for r in range(8):
            for c in range(8):
                piece = game.board[r][c]
                if not piece or piece.colour != 'b': continue
                moves = move_cache.get((r,c), game.get_moves(r,c))
                for mv in moves:
                    if game.board[mv[0]][mv[1]] and static_exchange_eval_local(game, mv, 'b', move_cache) < 0:
                        continue
                    copy = game.light_copy()
                    copy.make_move((r,c), mv)
                    val = minimax_sse(copy, depth-1, alpha, beta, True, original_depth, move_cache)
                    if val < min_eval:
                        min_eval = val; best_move = ((r,c), mv)
                    beta = min(beta, val)
                    if beta <= alpha: break
        return best_move if depth == original_depth else min_eval

# Helper which wrapper
def shutil_which(cmd):
    return shutil.which(cmd)

# Enhanced build_openings_npz supporting JSON/CSV/TSV/TSV.GZ/YAML
def build_openings_npz(openings_dir='data/chess-openings', out_npz='openings_dataset.npz', max_openings=None):
    d = Path(openings_dir)
    if not d.exists():
        raise FileNotFoundError(f"{openings_dir} not found. Run download_openings_git() or clone manually.")

    candidates = (list(d.rglob('*.json')) + list(d.rglob('*.csv')) +
                  list(d.rglob('*.tsv')) + list(d.rglob('*.tsv.gz')) +
                  list(d.rglob('*.yaml')) + list(d.rglob('*.yml')))
    if not candidates:
        raise RuntimeError("No JSON/CSV/TSV/YAML files found in the openings directory.")

    chosen = None
    for c in candidates:
        if c.name.lower() in ('openings.json','openings.csv','openings.tsv','openings.tsv.gz','openings.yaml','openings.yml'):
            chosen = c
            break
    if chosen is None:
        chosen = candidates[0]

    moves = []
    ecos = []
    names = []
    print(f"Parsing openings from {chosen}")

    suffix = chosen.suffix.lower()
    if chosen.name.lower().endswith('.tsv.gz'):
        suffix = '.tsv.gz'

    if suffix == '.json':
        with open(chosen, 'r', encoding='utf-8') as fh:
            data = json.load(fh)
        if isinstance(data, dict):
            data_iter = data.values()
        else:
            data_iter = data
        for entry in data_iter:
            if not isinstance(entry, dict):
                continue
            mv = entry.get('pgn') or entry.get('moves') or entry.get('uci') or entry.get('san') or entry.get('sequence')
            eco = entry.get('eco') or entry.get('code') or entry.get('ECO')
            name = entry.get('name') or entry.get('label') or entry.get('opening')
            if mv and eco:
                moves.append(mv); ecos.append(eco); names.append(name or '')
            if max_openings and len(moves) >= max_openings:
                break

    elif suffix == '.csv':
        with open(chosen, 'r', encoding='utf-8') as fh:
            rdr = csv.DictReader(fh)
            for row in rdr:
                mv = row.get('pgn') or row.get('moves') or row.get('uci') or row.get('san')
                eco = row.get('eco') or row.get('ECO') or row.get('code')
                name = row.get('name') or row.get('label')
                if mv and eco:
                    moves.append(mv); ecos.append(eco); names.append(name or '')
                if max_openings and len(moves) >= max_openings:
                    break

    elif suffix in ('.tsv', '.tsv.gz'):
        if suffix == '.tsv.gz':
            fh = gzip.open(chosen, 'rt', encoding='utf-8', errors='replace')
        else:
            fh = open(chosen, 'r', encoding='utf-8', errors='replace')
        with fh:
            rdr = csv.DictReader(fh, delimiter='\t')
            mv_keys = ['moves','pgn','uci','san','sequence','move']
            eco_keys = ['eco','code','ECO']
            name_keys = ['name','label','opening','title']
            for row in rdr:
                if not isinstance(row, dict):
                    continue
                mv = None; eco = None; name = None
                for k in mv_keys:
                    if k in row and row[k]:
                        mv = row[k]; break
                for k in eco_keys:
                    if k in row and row[k]:
                        eco = row[k]; break
                for k in name_keys:
                    if k in row and row[k]:
                        name = row[k]; break
                if mv is None:
                    first_col = next(iter(row.keys()), None)
                    mv = row.get(first_col) if first_col else None
                if eco is None:
                    keys = list(row.keys())
                    if len(keys) > 1:
                        eco = row.get(keys[1])
                if mv and eco:
                    moves.append(mv); ecos.append(eco); names.append(name or '')
                if max_openings and len(moves) >= max_openings:
                    break

    elif suffix in ('.yaml', '.yml'):
        try:
            import yaml
            with open(chosen, 'r', encoding='utf-8') as fh:
                data = yaml.safe_load(fh)
            if isinstance(data, dict): data_iter = data.values()
            else: data_iter = data
            for entry in data_iter:
                if not isinstance(entry, dict):
                    continue
                mv = entry.get('pgn') or entry.get('moves') or entry.get('uci') or entry.get('san') or entry.get('sequence')
                eco = entry.get('eco') or entry.get('code') or entry.get('ECO')
                name = entry.get('name') or entry.get('label') or entry.get('opening')
                if mv and eco:
                    moves.append(mv); ecos.append(eco); names.append(name or '')
                if max_openings and len(moves) >= max_openings:
                    break
        except Exception:
            raise RuntimeError("YAML file found but PyYAML not installed; install pyyaml or provide JSON/CSV/TSV.")

    else:
        raise RuntimeError(f"Unsupported file format: {chosen.suffix}")

    if not moves:
        raise RuntimeError("No openings parsed — file format may differ from expectations.")

    np.savez_compressed(out_npz, moves=np.array(moves, dtype=object),
                        eco=np.array(ecos, dtype=object), name=np.array(names, dtype=object))
    print(f"Saved openings dataset to {out_npz} ({len(moves)} entries)")
    return out_npz

# A minimal training routine (kept short)
def train_openings(npz_path, epochs=3, batch_size=64, lr=1e-3, out_dir='openings_checkpoints'):
    if not TORCH_AVAILABLE:
        raise RuntimeError("torch not installed")
    os.makedirs(out_dir, exist_ok=True)
    data = np.load(npz_path, allow_pickle=True)
    moves, ecos = data['moves'], data['eco']
    unique = list(dict.fromkeys(ecos.tolist()))
    label2idx = {lab:i for i,lab in enumerate(unique)}
    y = np.array([label2idx.get(e, -1) for e in ecos]); mask = y>=0
    moves, y = moves[mask], y[mask]
    vocab = {}; tokenized = []
    for s in moves:
        toks = str(s).split(); tokenized.append(toks)
        for t in toks:
            if t not in vocab:
                vocab[t] = len(vocab) + 1
    unk = len(vocab) + 1; vocab['<unk>'] = unk; vocab_size = unk + 1
    max_len = 8
    X = np.zeros((len(tokenized), max_len), dtype=int)
    for i,toks in enumerate(tokenized):
        for j,t in enumerate(toks[:max_len]):
            X[i,j] = vocab.get(t, unk)
    from torch.utils.data import Dataset, DataLoader
    class DS(Dataset):
        def __init__(self,X,y): self.X=torch.from_numpy(X).long(); self.y=torch.from_numpy(y).long()
        def __len__(self): return len(self.y)
        def __getitem__(self,i): return self.X[i], self.y[i]
    dl = DataLoader(DS(X,y), batch_size=batch_size, shuffle=True)
    class Net(nn.Module):
        def __init__(self, vs): super().__init__(); self.emb=nn.Embedding(vs,64,padding_idx=0); self.fc1=nn.Linear(64,128); self.fc2=nn.Linear(128,len(unique))
        def forward(self,x): e=self.emb(x).mean(1); return self.fc2(F.relu(self.fc1(e)))
    model = Net(vocab_size).to(DEVICE)
    opt = torch.optim.AdamW(model.parameters(), lr=lr)
    loss_fn = nn.CrossEntropyLoss()
    for ep in range(1, epochs+1):
        model.train(); total=0; n=0
        for xb,yb in dl:
            xb,yb = xb.to(DEVICE), yb.to(DEVICE)
            loss = loss_fn(model(xb), yb)
            opt.zero_grad(); loss.backward(); opt.step()
            total += loss.item(); n += 1
        print(f"Epoch {ep}: loss={total/max(1,n):.4f}")
        torch.save({'model': model.state_dict(), 'vocab': vocab, 'labels': unique, 'vocab_size': vocab_size},
                   os.path.join(out_dir, f'ckpt_epoch{ep}.pth'))
    print("Training done; checkpoints in", out_dir)

# Self-test helper
def selftest():
    os.makedirs('data/chess-openings', exist_ok=True)
    f = Path('data/chess-openings/openings.json')
    if not f.exists():
        json.dump([
            {'pgn':'1. e4 e5 2. Nf3 Nc6','eco':'C20','name':'Open Game'},
            {'pgn':'1. d4 d5 2. c4','eco':'D00','name':'QG'},
            {'pgn':'1. e4 c5','eco':'B20','name':'Sicilian'}
        ], open(f,'w'), indent=2)
        print('Created sample openings.json')
    npz = build_openings_npz('data/chess-openings', 'openings_dataset.npz')
    if TORCH_AVAILABLE:
        try:
            train_openings(npz, epochs=1, batch_size=2, lr=1e-3, out_dir='checkpoints_test')
        except Exception as e:
            print("Training failed:", e)
    else:
        print('Torch unavailable — skipped training.')
    print('Selftest complete.')

if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument('--selftest', action='store_true')
    args = p.parse_args()
    if args.selftest:
        selftest()




